# -*- coding: utf-8 -*-
"""Gathering Tweets

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rdYc2igfblVuoC732DWj2NZ8llnqEjO
"""

import nltk
import sys
nltk.download('averaged_perceptron_tagger')
from nltk import pos_tag, Tree
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
import nltk.tokenize as tokenizer
from nltk.tag import StanfordNERTagger
import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
import os
import spacy
import neuralcoref


nltk.download('punkt')
nltk.download('wordnet')

import en_core_web_sm

# Tokenize paragraph into sentences
def tokenize_sentence(paragraph):
    sentence_list = sent_tokenize(paragraph)
    return sentence_list

# Tokenize sentences into words
def tokenize_words(sentence):
    token_list = word_tokenize(sentence)
    return token_list

# Stemming the words
def stem_word(word):
    stemmer = PorterStemmer()
    return stemmer.stem(word)

# Lemmatize the words
def lemmatize_words(sentences):
    lemma_list = []
    lemmatizer = WordNetLemmatizer()
    for words in sentences:
        for w in words:
            lemma = lemmatizer.lemmatize(w, 'v')
            lemma_list.append(lemma)
    return lemma_list

# Finding Part-of-Speech(POS) tags for the words
def get_pos_tags(tokens):
    tags = pos_tag(tokens)
    return tags

# Finding parse tree for syntactic/dependency parsing of sentences
def generate_parse_tree(node):
    if node.n_lefts + node.n_rights > 0:
        return Tree(node.orth_, [generate_parse_tree(child) for child in node.children])
    else:
        return node.orth_

# Extracting hypernyms
def get_hypernyms(word):
    hypernyms = []
    for ss in wordnet.synsets(word):
        for hyper in ss.hypernyms():
            hypernyms.append(hyper.name().split('.')[0])
    return hypernyms

# Extracting hyponyms
def get_hyponyms(word):
    hyponyms = []
    for ss in wordnet.synsets(word):
        for hypo in ss.hyponyms():
            hyponyms.append(hypo.name().split('.')[0])
    return hyponyms

# Extracting meronyms
def get_meronyms(word):
    meronyms = []
    for ss in wordnet.synsets(word):
        for mero in ss.part_meronyms():
               meronyms.append(mero.name().split('.')[0])
    return meronyms

# Extracting holonyms
def get_holonyms(word):
    holonyms = []
    for ss in wordnet.synsets(word):
        for holo in ss.part_holonyms():
            holonyms.append(holo.name().split('.')[0])
    return holonyms

# Extracting synonyms
def get_synonyms(word):
    synonym = []
    for ss in wordnet.synsets(word):
        for l in ss.lemmas():
            synonym.append(l.name())
    synonym_set = set(synonym)
    synonym = list(synonym_set)
    return synonym

# User options
def print_on_console():
    print("Press 0 :: Continue to information extraction")
    print("Press 1 :: Tokenize sentences into words")
    print("Press 2 :: Lemmatize the words")
    print("Press 3 :: Find POS Tags of words")
    print("Press 4 :: Find parse tree for dependency/syntactic parsing")
    print("Press 5 :: Find hypernyms, hyponyms, meronyms, holonyms")
    choice = input("Enter your choice :: ")
    return choice

def getPartWords():
    file=open('worldcities.csv','rb')
    eachLine = file.readline().decode('utf-8','ignore')
    eachLine = file.readline().decode('utf-8','ignore')
    place=[]
    while(eachLine):
        data=eachLine.split(",")
        place.append(data[1].replace('"',''))
        country=data[4].replace('"','')
        state=data[7].replace('"','')
        if(country not in place):
            place.append(country)
        if(state not in place and state!=''):
            place.append(state)
        eachLine = file.readline().decode('utf-8','ignore')
    return place

def getWorkWords():
    template_words=['work','ceo',
    'chief','vice','chairman','former',
    'mayor','commissioner','faculty','found',
    'chair','secretary',
    'minister','officer','member','serve',
    'board','specialist','coordinator',
    'architect','analyst','assistant','salesman','magician']
    f=open("jobTitle.txt","r")
    for eachLine in f:
        template_words.append(eachLine.strip())
    f.close()
    return template_words

# Get related words for all template
def get_all_related_template_words(all_templates):
    all_words = []
    lemmatizer = WordNetLemmatizer()

    for template in all_templates:
        if(template=="BUY"):
            template_words = ["acquire","buy","purchase"]
        elif(template=="WORK"):
            template_words=workWords
        all_words.append((template, template_words))
    return all_words

# Match a sentence to a template
def template_matching(roots):
    matched_templates = []
    lemmatizer = WordNetLemmatizer()
    for root in roots:
        root = lemmatizer.lemmatize(root, 'v')
        for possible_words in all_possible_words:
            if root in possible_words[1]:
                matched_templates.append(possible_words[0])
    return matched_templates

# Get relations
def get_spans(doc):
    spans = list(doc.ents) + list(doc.noun_chunks)
    for span in spans:
        span.merge()

# BUY

# Buyer relation
def extract_buyer_relation(NerSpacy,sentence):
  if (("for purchase" not in sentence) and ("for purchasing" not in sentence) and ("for buying" not in sentence) and ("for acquiring" not in sentence)):
    for token in NerSpacy:
      if(token[1]=="ORG" or token[1]=="PERSON"):
          return token[0]
  return ""    

# Item relation
def extract_item_relation(buyer,NerSpacy,lemmaTokens,posTags):
    ## to be coded
    for token in NerSpacy:
      if(token[1]=="PRODUCT"):
          return token[0]
    for token in NerSpacy:
      if(token[1]=="ORG" and token[0]!=buyer ):
          return token[0]
    buyW=["acquire","buy","purchase"]
    idx=-1
    for eachW in buyW:
      if(eachW in lemmaTokens):
        idx=lemmaTokens.index(eachW)
        break;

    word=tokens[idx]
    for  i in range(len(posTags)):
          if(posTags[i][0]==word):
            idx=i;
            break;
          
    return getItemName(i,posTags)

def getItemName(i,posTags):
  word=posTags[i+1][0];
  if(word=='by' or word=='BY'):
    return posTags[i-2][0]

  word=posTags[i+1][1]
  
  name=posTags[i+1][0]

  next=i+2
  nextWordTag=posTags[next][1];
  while(nextWordTag=="JJS" or nextWordTag=="JJR" or nextWordTag=="JJ" or nextWordTag=="NN" or nextWordTag=="NNP" or nextWordTag=="NNPS" or nextWordTag=="NNS" ):
    name+=" "+posTags[next][0]
    next+=1
    nextWordTag=posTags[next][1];
  return name.strip()


    

# extract location event relation
def extract_price_relation(NerSpacy,tokens):
    for token in NerSpacy:
      if(token[1]=="MONEY"):
          return token[0]
    try:
      i=tokens.index('$')
      try:
        return tokens[i+1]
      except: 
        return tokens[i-1]
    except:
      return ""
# Quantity relation
def extract_quantity_relation(NerSpacy,item,tokens,posTags):
  for token in NerSpacy:
      if(token[1]=="QUANTITY"):
          return token[0]
  if(item==""):
    return ""
  else:
    Item=item.split()[0]
    idx=-1;
    for  i in range(len(posTags)):
      if(posTags[i][0]==Item):
        idx=i;
    if(idx!=-1):
      if(posTags[idx][1]=='CD'):
        return tokens[i-1]
    
    return ""

# Source relation
def extract_source_relations(doc):
    get_spans(doc)
    relations = []
    ## to be coded
    return relations

# WORK

# Person relation
def extract_person_relations(NerSpacy):
  person="";
  for token in NerSpacy:
      if(token[1]=="PERSON"):
          person+=token[0]+","
  return person[:-1]
    
# Organization relation
def extract_organization_relation(NerSpacy):
  for token in NerSpacy:
      if(token[1]=="ORG"):
          return token[0]
  return ""

# Position relation
def extract_position_relation(getWorkWords,lemmaTokens,tokens,posTags):
  work=[]
  for eachJob in getWorkWords:
    if(eachJob in lemmaTokens):
      i=lemmaTokens.index(eachJob)
      work.append(i)
  position=""
  for eachIndex in work:
    position+=tokens[eachIndex]+" "
  return position.strip()
# Location relation
def extract_location_relation(NerSpacy):
  location=""
  for token in NerSpacy:
      if(token[1]=="NORP" or token[1]=="GPE" or  token[1]=="LOC" or token[1]=="FAC"  ):
          location+=token[0]+" "
  return location.strip()
    

# PART

# Location-Location relation

def checkForLocation(locationWords,sentence,lemmaTokens,posTags,NerSpacy):
	locations=[]
  
	for eachToken in NerSpacy:
		if(eachToken[1]=="GPE" or eachToken[1]=="LOC" ):
			if eachToken[0] not in locations:
				locations.append(eachToken[0])
	for token in tokens:
		if token in locationWords:
			if token not in locations:
				locations.append(token)
	if(len(locations)>3):
		return ""

	indx=[]
	try:
		for eachLoc in locations:
			indx.append(tokens.index(eachLoc.split()[0]))
	except:
		pass

	partFrame=[]
	for i in range(len(indx)):
		for j in range(len(indx)):
			if(i!=j):
				try:
					if(tokens[indx[i]+1]==',' and tokens[indx[j]-1]==","):
						partFrame.append([locations[i],locations[j]])
					elif(tokens[indx[i]+2]=="in" and tokens[indx[i]+3]=="the" and tokens[indx[j]-2]=="in" and tokens[indx[j]-1]=="the"):
						partFrame.append([locations[i],locations[j]])
					elif(tokens[indx[i]+1]=="in" and tokens[indx[j]-1]=="in" ):
						partFrame.append([locations[i],locations[j]])
					elif((tokens[indx[i]+1]=="is" or tokens[indx[i]+1]=="was" or tokens[indx[i]+1]=="are") and tokens[indx[j]-1]=="in"):
						partFrame.append([locations[i],locations[j]])
					elif(tokens[indx[i]+2]=="in" and tokens[indx[j]-1]=="in"):
						partFrame.append([locations[i],locations[j]])
					elif(tokens[indx[i]+2]=="part" and tokens[indx[i]+3]=="of" and tokens[indx[j]-2]=="part" and tokens[indx[j]-1]=="of"):
						partFrame.append([locations[i],locations[j]])
				except:
					pass
	return partFrame



def extract_location_location_relation(doc):
    get_spans(doc)
    relations = []
    ## to be coded
    return relations

def printNEREntities(NERText,File):
    for NERtag in NERText:
        if(NERtag[1]=="PERSON"):
            File.write("Person: ")
            File.write(NERtag[0]+"\n")
        elif(NERtag[1]=="LOCATION"):
            File.write("Location: ")
            File.write(NERtag[0]+"\n")
        elif(NERtag[1]=="ORGANIZATION"):
            File.write("Organization: ")
            File.write(NERtag[0]+"\n")

def toJson(my_array):
    pairs = zip(my_array[1], my_array[0])
    json_values = ('"{}": {}'.format(label, value) for label, value in pairs)
    my_string = '{' + ', '.join(json_values) + '}'
    return my_string

def addHeader(fileName):
    global jsonFile
    
    jsonFile.write('{"document": "'+fileName+'",\n"extraction": [\n')
    
def addFooter():
    global jsonFile
    jsonFile.write(']\n}')
    jsonFile.close()
def addTOJson(my_array2): 
    global jsonFile
    global firstTime
    template_name = my_array2[1][0]
    arg=my_array2[1]
    args=[]
    header=[]
    for i in range(1,len(arg)):
        if(arg[i]!=""):
            args.append('"'+str(arg[i])+'"')
        else:
            args.append('""')
        header.append(str(i))

    my_array = [args,header]
    js=toJson(my_array)

    pHeader=['template','sentences','arguments']
    sen=my_array2[0]
    data=['"'+template_name+'"','"'+sen+'"',js]

    my_strcut=[data,pHeader] 
    f=toJson(my_strcut)
    if(firstTime==True):
        firstTime=False
        jsonFile.write(f)
    else:
        jsonFile.write("\n,"+f)


#============================================

if __name__ == "__main__":

    isTokensized=False
    path=str(sys.argv[1])    
    nlp = en_core_web_sm.load()
    fileName=path


    file= open('WikipediaArticles/'+fileName,'rb')

    coref = spacy.load('en_core_web_sm')
    # Let's try before using the conversion dictionary:
    neuralcoref.add_to_pipe(coref)
    corefile= open("corefResolved"+fileName,'w',encoding="utf-8")
    eachLine = file.readline().decode('utf-8','ignore')
    while(eachLine):
      line = coref(eachLine)
      corefile.write(line._.coref_resolved)
      eachLine = file.readline().decode('utf-8','ignore')
    file.close()
    corefile.close()
    file= open('corefResolved'+fileName,'rb')
    
    buyFile = open("buy.txt","w",encoding="utf=8")
    workFile = open("work.txt","w",encoding="utf=8")
    partFile = open("part.txt","w",encoding="utf=8")
    elseFile = open("else.txt","w",encoding="utf=8")
    jsonFile=open("Templates.json","w",encoding="utf-8")
    firstTime=True;
    addHeader(fileName)

    
    workWords = getWorkWords()
    partWords = getPartWords()
    eachLine = file.readline().decode('utf-8','ignore')
    while(eachLine):
        sentences = tokenizer.sent_tokenize(eachLine)
        for sentence in sentences: 
            doc_nlp = nlp(str(sentence))
          
            sentences = []
            words = []
            lemmas = []
            pos_tags = []
            all_hypernyms = []
            all_hyponyms = []
            all_meronyms = []
            all_holonyms = []

            
            
            # Template Extraction and Filling
            templates_dict = {'BUY': {'Buyer': '', 'Item': '', 'Price': '', 'Quantity': '', 'Source': ''},
                            'WORK': {'Person': '', 'Organization': '', 'Position': '', 'Location': ''}
                            }

            template_names = []
            for key in templates_dict:
                template_names.append(key)
            
            all_possible_words = get_all_related_template_words(template_names)
            roots = []
            """ for tree in doc_nlp.sents:
                roots.append(tree.root.orth_)
            """
            """ for word_tag in doc_nlp:
                if word_tag.pos_ == 'NOUN' or word_tag.pos_ == 'VERB':
                    roots.append(word_tag.text) """
            tokens=tokenize_words(sentence)
            matched_templates = template_matching(tokens,)
            matched_set = set(matched_templates)
            matched_templates = list(matched_set)
            lemmatizer = WordNetLemmatizer()
            NerSpacy=[(X.text, X.label_) for X in doc_nlp.ents]

            lemmaTokens=[]
            for word in tokens:
              lemmaTokens.append(lemmatizer.lemmatize(word,'v'))
            posTags=get_pos_tags(tokens)


            partTemplate=checkForLocation(getPartWords(),sentence,lemmaTokens,posTags,NerSpacy)
            if(len(partTemplate)>0):
              matched_templates.append('PART')
              for template in partTemplate:
                partFile.write(str(template)+"\n")

                partArray = [sentence.replace('"',"'"),["Part",str(template[0]),str(template[1])]]
                addTOJson(partArray)


            if len(matched_templates) == 0:
                elseFile.write(sentence+"\n")
                print(" No matching template found. Enter sentences related to the domain.")

            for template in matched_templates:
                rel = []
                if template == 'BUY':
                    buyer=extract_buyer_relation(NerSpacy,sentence)
                    if(buyer!=""):
                      buyFile.write(sentence+"\n")
                      item=extract_item_relation(buyer,NerSpacy,lemmaTokens,posTags)
                      price=extract_price_relation(NerSpacy,tokens)
                      quantity=extract_quantity_relation(NerSpacy,item,tokens,posTags)
                      buyRecord="BUY {Buyer: "+str(buyer)+", Item: "+str(item)+", Price: "+str(price)+", Quantity: "+str(quantity)+" , Source: ___ }"
                      print(buyRecord)
                      buyFile.write(buyRecord+"\n")
                      buyArray = [sentence.replace('"',"'"),["Buy",str(buyer),str(item),str(price),str(quantity),""]]
                      addTOJson(buyArray)

                      



                    
                    #nltk_tree = [generate_parse_tree(sent.root).pretty_print() for sent in doc_nlp.sents]
                    #print(nltk_tree)


                    ## to be coded

                if template == 'WORK':
                    person=extract_person_relations(NerSpacy)
                    if(person!=""):
                      workFile.write(sentence+"\n")
                      organization=extract_organization_relation(NerSpacy)
                      position=extract_position_relation(getWorkWords(),lemmaTokens,tokens,posTags)
                      location=extract_location_relation(NerSpacy)
                      workRecord="WORK {Person: "+person+", Organization: "+organization+", Position: "+position+", Location: "+location+" }"
                      print(workRecord)
                      workFile.write(workRecord+"\n")
                      workArray = [sentence.replace('"',"'"),["Work",str(person),str(organization),str(position),str(location)]]
                      addTOJson(workArray)




                    


                    ## to be coded

                

                    ## to be coded
        eachLine = file.readline().decode('utf-8','ignore')
    addFooter()
    buyFile.close()
    workFile.close()
    partFile.close()

